{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import pandas as pn\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "filename='new_pairs'\n",
    "class_num=35\n",
    "iter_num=5\n",
    "use_pickle=True\n",
    "store_matrices=False\n",
    "\n",
    "# open file - read verb nouns    \n",
    "with codecs.open(filename,\"r\",\"utf-8\") as f:\n",
    "    f_lines = f.readlines()\n",
    "\n",
    "\n",
    "# split verb and noun\n",
    "f_lines = [f_lines[i].encode(\"utf-8\").lower().rstrip().split(None,1) for i in range(len(f_lines))]\n",
    "f_lines = [i for i in f_lines if '_s_' in i[0] or '_so_' in i[0]]\n",
    "verbs=[]\n",
    "for i in f_lines:\n",
    "    verbs.append(i[0].lower().rstrip())\n",
    "verbs_freqs=Counter(verbs)\n",
    "f_lines = [i for i in f_lines if not (verbs_freqs[i[0]]<11)]\n",
    "     \n",
    "# first implementation of the frequencies counter        \n",
    "pairs = [str(f_lines[i][0])+' '+str(f_lines[i][1]) for i in range(len(f_lines))]\n",
    "pairs = Counter(pairs)\n",
    "\n",
    "# verbs and nouns lists with all the verbs and nouns\n",
    "verbs = []\n",
    "nouns = []\n",
    "for i in f_lines:\n",
    "    verbs.append(i[0].lower().rstrip())\n",
    "    nouns.append(i[1].lower().rstrip())\n",
    "\n",
    "verbs_freqs = Counter(verbs)  \n",
    "nouns_freqs = Counter(nouns)\n",
    "unique_verbs = list(set(verbs))\n",
    "unique_nouns = list(set(nouns))\n",
    "\n",
    "verbs_dict = {}\n",
    "nouns_dict = {}\n",
    "for i in f_lines:\n",
    "    i[0] = i[0].lower().rstrip()\n",
    "    i[1] = i[1].lower().rstrip()\n",
    "    \n",
    "    if i[0] in verbs_dict:\n",
    "        if i[1] in verbs_dict[i[0]]:\n",
    "            verbs_dict[i[0]][i[1]] += 1\n",
    "        else:\n",
    "            verbs_dict[i[0]].update({i[1] : 1})\n",
    "        \n",
    "    else:\n",
    "        verbs_dict[i[0]] = {i[1] : 1}\n",
    "        \n",
    "    if i[1] in nouns_dict:\n",
    "        if i[0] in nouns_dict[i[1]]:\n",
    "            nouns_dict[i[1]][i[0]] += 1\n",
    "        else:\n",
    "            nouns_dict[i[1]].update({i[0] : 1})\n",
    "        \n",
    "    else:\n",
    "        nouns_dict[i[1]] = {i[0] : 1}\n",
    "\n",
    "\n",
    "if use_pickle==True:\n",
    "    \n",
    "    file_p_c='second_phase_final_pairs/p_c_50.pkl'\n",
    "    file_p_n_c='second_phase_final_pairs/p_n_c_50'\n",
    "    file_p_v_c='second_phase_final_pairs/p_v_c_50'\n",
    "    file_pairs_classes='second_phase_final_pairs/pairs_classes50'\n",
    "\n",
    "    p_c = pickle.load(open(file_p_c, 'rb'))\n",
    "    p_n_c = pickle.load(open(file_p_n_c, 'rb'))\n",
    "    p_v_c = pickle.load(open(file_p_v_c, 'rb'))\n",
    "    pairs_classes = pickle.load(open(file_pairs_classes, 'rb'))\n",
    "    \n",
    "else:\n",
    "        \n",
    "    # EM for verb-noun-pair classes\n",
    "\n",
    "    pairs_classes = pn.DataFrame.from_items([(pair,[pairs[pair]/class_num]*class_num) for pair in pairs],orient='index', columns=[i for i in range(0,class_num)])\n",
    "\n",
    "    p_v_c = pn.DataFrame(np.random.uniform(0,1,size=(class_num,len(set(verbs)))), columns=[v for v in set(verbs)])\n",
    "\n",
    "    p_n_c = pn.DataFrame(np.random.uniform(0,1,size=(class_num,len(set(nouns)))), columns=[n for n in set(nouns)])\n",
    "\n",
    "\n",
    "    p_c = [1.0/class_num]*class_num\n",
    "    \n",
    "    # E-Step\n",
    "\n",
    "    for i in range(1,iter_num+1):\n",
    "        print i\n",
    "\n",
    "        for pair in pairs:\n",
    "\n",
    "            prod_classes_verbs_nouns = [a*b*c for a,b,c in zip(p_c,p_v_c[pair.split(None,1)[0]].tolist(),p_n_c[pair.split(None,1)[1]].tolist())]\n",
    "            norm = sum(prod_classes_verbs_nouns)\n",
    "\n",
    "            for class_ in range(len(p_c)):\n",
    "\n",
    "                pairs_classes.set_value(pair,class_,(p_c[class_] * p_v_c[pair.split(None,1)[0]][class_] * p_n_c[pair.split(None,1)[1]][class_])/norm)\n",
    "\n",
    "\n",
    "        # M Step\n",
    "\n",
    "        # Calculate sum_over_y(f(y)*p_theta(x|y))\n",
    "        y=[]\n",
    "        for class_ in xrange(class_num):\n",
    "            probs=[]\n",
    "            for verb in verbs_dict:\n",
    "                for noun in verbs_dict[verb]:\n",
    "                    probs.append(verbs_dict[verb][noun] * pairs_classes.at[str(verb) + ' ' + str(noun),class_])\n",
    "            y.append(sum(probs))\n",
    "\n",
    "        # M(theta_vc)\n",
    "        for class_ in xrange(class_num):\n",
    "            for verb in verbs_dict:\n",
    "                probs=[]\n",
    "                for noun in verbs_dict[verb]:\n",
    "                    probs.append(verbs_dict[verb][noun] * pairs_classes.at[str(verb) + ' ' + str(noun),class_])\n",
    "                p_v_c.at[class_,str(verb)] = sum(probs) / y[class_]\n",
    "\n",
    "        # M(theta_nc)\n",
    "        for class_ in xrange(class_num):\n",
    "            for noun in nouns_dict:\n",
    "                probs=[]\n",
    "                for verb in nouns_dict[noun]:\n",
    "                    probs.append(nouns_dict[noun][verb] * pairs_classes.at[str(verb) + ' ' + str(noun),class_])\n",
    "                p_n_c.at[class_,str(noun)] = sum(probs) / y[class_]\n",
    "\n",
    "        # M(theta_c)\n",
    "        for class_ in xrange(class_num):\n",
    "            p_c[class_] = y[class_] / len(pairs_classes)\n",
    "\n",
    "        # Store results\n",
    "        \n",
    "        if store_matrices==True:\n",
    "        \n",
    "            p_v_c.to_pickle('p_v_c/p_v_c_'+str(i))\n",
    "            p_n_c.to_pickle('p_n_c/p_n_c_'+str(i))\n",
    "            with open('p_c/p_c_'+str(i)+'.pkl','wb') as f:\n",
    "                pickle.dump(p_c,f)\n",
    "            pairs_classes.to_pickle('pairs_classes/pairs_classes'+str(i))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get hard verb classes\n",
    "\n",
    "v_c = {}\n",
    "\n",
    "for verb in verbs_dict:\n",
    "    class_=p_v_c[verb].idxmax()\n",
    "    if class_ in v_c:\n",
    "        v_c[class_].update({verb:p_v_c[verb].max()})\n",
    "    else:\n",
    "        v_c[class_]={verb:p_v_c[verb].max()}\n",
    "\n",
    "for class_ in xrange(class_num):    \n",
    "    v_c[class_]=sorted(v_c[class_].items(),key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Show highest ranked verbs for each class (soft classes)\n",
    "#for class_ in xrange(class_num):\n",
    "#    print class_\n",
    "#    print p_v_c.iloc[class_].sort_values(ascending=False)[:20]\n",
    "\n",
    "# Get hard noun classes\n",
    "\n",
    "n_c = {}\n",
    "\n",
    "for noun in nouns_dict:\n",
    "    class_=p_n_c[noun].idxmax()\n",
    "    if class_ in n_c:\n",
    "        n_c[class_].update({noun:p_n_c[noun].max()})\n",
    "    else:\n",
    "        n_c[class_]={noun:p_n_c[noun].max()}\n",
    "\n",
    "for class_ in xrange(class_num):    \n",
    "    n_c[class_]=sorted(n_c[class_].items(),key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting intransitive verbs\n",
    "\n",
    "intr_verbs={}\n",
    "for verb in verbs_dict:\n",
    "    if '_s_' in verb:\n",
    "        intr_verbs[verb] = verbs_dict[verb]\n",
    "\n",
    "# Calc p(n)\n",
    "\n",
    "p_n = {}\n",
    "for verb in intr_verbs:\n",
    "    for noun in intr_verbs[verb]:\n",
    "        tmp=[]\n",
    "        for class_ in xrange(class_num):\n",
    "            tmp.append(p_c[class_]*p_n_c.at[class_,noun])\n",
    "        if verb in p_n:\n",
    "            p_n[verb].update({noun:sum(tmp)})\n",
    "        else:\n",
    "            p_n[verb]={noun:sum(tmp)}\n",
    "\n",
    "# Estimate           \n",
    "\n",
    "verb_noun_rank = {}\n",
    "verb_noun_value={}\n",
    "threshold=10\n",
    "for verb in intr_verbs:\n",
    "    tmp=[]\n",
    "    tmp2=[]\n",
    "    verb_class = p_v_c[verb].idxmax()\n",
    "    for noun in intr_verbs[verb]:\n",
    "        p_theta=(p_c[verb_class] * p_n_c.at[verb_class,noun]) / p_n[verb][noun]\n",
    "        tmp.append(intr_verbs[verb][noun] * p_theta)\n",
    "        tmp2.append(intr_verbs[verb][noun])\n",
    "        if verb in verb_noun_rank:\n",
    "            verb_noun_rank[verb].update({noun:intr_verbs[verb][noun] * p_theta})\n",
    "        else:\n",
    "            verb_noun_rank[verb]={noun:intr_verbs[verb][noun] * p_theta}\n",
    "    verb_noun_rank[verb]=sorted(verb_noun_rank[verb].items(),key=lambda x: x[1], reverse=True)[:threshold]\n",
    "    verb_noun_value[verb]=(sum(tmp)/sum(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting transitive verbs\n",
    "\n",
    "trans_verbs={}\n",
    "for verb_s in verbs_dict:\n",
    "    if '_so_' in verb_s and 'nsubj' in verb_s:\n",
    "        for verb_o in verbs_dict:\n",
    "            if '_so_' in verb_o and 'dobj' in verb_o:\n",
    "                if verb_s.split('_')[0] == verb_o.split('_')[0]:\n",
    "                    trans_verbs[(str(verb_s),str(verb_o))]=[(verbs_dict[verb_s],verbs_dict[verb_o])]\n",
    "\n",
    "# Get p(n_1,n_2)\n",
    "\n",
    "p_n1_n2 = {}\n",
    "\n",
    "for verb in trans_verbs:\n",
    "    tmp=[]\n",
    "    for noun1 in trans_verbs[verb][0][0]:\n",
    "        for noun2 in trans_verbs[verb][0][1]:\n",
    "            for class1 in xrange(class_num):\n",
    "                for class2 in xrange(class_num):\n",
    "                    tmp.append((p_c[class1]*p_c[class2])*p_n_c.at[class1,noun1]*p_n_c.at[class2,noun2])\n",
    "            if verb in p_n1_n2:\n",
    "                p_n1_n2[verb].update({(noun1,noun2):sum(tmp)})\n",
    "            else:\n",
    "                p_n1_n2[verb]={(noun1,noun2):sum(tmp)}\n",
    "\n",
    "# Estimate\n",
    "\n",
    "verb_tr_noun_rank={}\n",
    "threshold=10\n",
    "for verb in p_n1_n2:\n",
    "    verb_class1=p_v_c[verb[0]].idxmax()\n",
    "    verb_class2=p_v_c[verb[1]].idxmax()\n",
    "    for noun1,noun2 in p_n1_n2[verb].keys():\n",
    "        p_theta=(p_c[verb_class1] * p_c[verb_class2] * p_n_c.at[verb_class1,noun1] * p_n_c.at[verb_class2,noun2]) / p_n1_n2[verb][(noun1,noun2)] # Not correct\n",
    "        if verb in verb_tr_noun_rank:\n",
    "            verb_tr_noun_rank[verb].update({(noun1,noun2):verbs_dict[verb[0]][noun1]+verbs_dict[verb[1]][noun2] * p_theta})\n",
    "        else:\n",
    "            verb_tr_noun_rank[verb]={(noun1,noun2):verbs_dict[verb[0]][noun1]+verbs_dict[verb[1]][noun2] * p_theta}\n",
    "    verb_tr_noun_rank[verb]=sorted(verb_tr_noun_rank[verb].items(),key=lambda x: x[1], reverse=True)[:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
